# Introduction 

Reinforcement Learning (RL) is fundamentally about learning to take right actions from data, whether that data is obtained online or offline. This course adopts a broader perspective on RL than is typically presented in other textbooks. Rather than emphasizing only the trial-and-error aspect of RL, which has been historically prominent due to the pioneering work of Sutton and Barto, we view RL as a more general paradigm which seeks to transform data into useful policies. This perspective allows us to consider a variety of techniques encompassing optimal control, system identification methods, and ideas from simulation, in addition to the usual machinery of temporal difference learning.

Traditional RL research often prioritizes "solving" benchmark problems within pre-defined environments. While valuable for theoretical exploration, this narrow focus neglects the crucial step of formulating these environments themselves. This creates a gap between RL as a powerful tool for addressing real-world challenges and its current position as an idealized theoretical model of human intelligence. {cite:t}`Iskhakov2020` echo this sentiment when highlighting that the "daunting problem" hindering wider adoption of decision-making algorithms in econometrics is "the difficulty of learning about the objective function and environment facing real-world decision-makers". When venturing beyond academia, numerous challenges beyond choosing an RL algorithm emerge: defining time units (discrete, continuous, start points, durations), identifying states, determining actions, and understanding their evolution over time. Crucially, defining the objective the system should achieve is non-trivial as real-world systems interacting with humans demand factoring in preferences, values, and judgments that even domain experts often struggle to formalize. 

## The Reinforcement Learning Problem 

Reinforcement Learning is a field that has evolved from its roots in animal psychology and AI in the 1980s. However, it doesn't have exclusivity on the problem of figuring out how to make good decisions automatically. This broader question has been a burgeoning topic during and post-World War II, with significant efforts put into ballistics and resource planning. These advances led to the creation of different lines of research, including control theory and operations research (OR). While not explicitly called "learning," both control theory and OR have also developed learning ideas through the concepts of adaptation (with strong influence from Wiener's cybernetics) and system identification.

But what is learning, after all? While I won't venture into crafting a perfect definition for the term, I think we can all agree that learning involves data, i.e., measurements of some phenomenon of interest. However, data alone is not enough; we must do something with it. In the decision-making context, what we are mostly interested in doing with that data is refining the quality of our decisions: to adapt. Supervised learning, the main flavor of machine learning in our everyday lives, also involves data and a process of adapting the model's weights. So, how can we cast the problem of learning how to make good decisions from data as a supervised learning problem?

One approach is to ask experts to provide demonstrations of what the right thing to do is in many different situations. Using supervised learning methods, we would then hope that our model will be capable of filling in the blanks and generalizing to situations that haven't been seen in the dataset. However, this approach, called imitation learning (IL), suffers from two main challenges: the availability of quality data and sufficient data coverage (quantity). Often, the very reason we want to develop a learning-based decision-making system (i.e., an RL agent) is that we either don't know how to solve the problem of interest well at all (hence can't provide demonstrations) or want to outperform what has been shown in the dataset (superhuman AI).

This is where we need a different approach to the problem. Rather than requiring input-output pairs for imitation, we will instead ask the system designer to provide a description of what they want to achieve through a mathematical function of a particular form (typically a sum of costs or rewards) and a description of the "laws of the world" by which this objective can be achieved (the dynamics). The bet we are making is that it will be easier for us to derive (through our algorithms) good decisions from this description than through the more direct IL approach. Metaphorically, the choice of either demonstrations or reward/dynamics pair is as if we were specifying a "basis" for inducing new and better ways of acting. However, not all bases are equally suitable for expressing complex problems, and therefore, a significant part of the work that a practitioner might end up spending is on this step: figuring out how to express what they want to do to the machine.

To appreciate the complexity of specifying those quantities, consider the complex task of dynamically adjusting temperature setpoints in a building to minimize energy usage while maintaining thermal comfort and adhering to equipment limitations. Defining this problem is challenging because thermal comfort preferences vary greatly between individuals and are impacted by elements like humidity, which sensors might not always accurately capture. Furthermore, responsible deployment demands safeguards, as pushing close to equipment limitations could cause service interruption, damage trust, and hinder user adoption. To add to the challenge, practical knowledge about controlling building HVAC systems is often held tacitly by experienced building managers and passed down primarily through on-the-job interactions and undocumented practices, presenting a potential knowledge loss with staff turnover. This very same challenge, understanding the complex decision-making preferences of human operators, also arises in other industries. Take water treatment plants, where the overarching goal is to maintain sufficient water outflow under fluctuating demands while simultaneously controlling turbidity and pH levels. This task necessitates difficult-to-quantify factors related to the operator's forecasting, risk perception, and overall intuition about the plant's state.

It is this need for understanding what people want that drives the field of preference elicitation, which goes hand-in-hand with that of inferring optimal ways of acting. In preference modeling, we are typically interested in determining preferences from data (typically of human provenance) obtained through a process of preference elicitation. Preference modeling is, therefore, also a learning problem, one which is closely related to supervised learning. However, the difference is that we are interested in obtaining a preference ordering from that data, and this ordering is not directly represented in the data, hence the need for inference. Preference data is typically obtained in a pairwise fashion, and under the right conditions in the Von Neumann theorem, those preferences are preserved by their equivalent representation as a scalar objective function: the utility function. It is under this form as a utility function inferred from pairwise preferences that many state-of-the-art Large Language Models (LLMs) are currently being developed, a methodology referred to as Reinforcement Learning from Human Feedback (RLHF). Compared to IL, this approach shines in the ease with which one can collect supervisory data: preferences tend to be easier to harvest at scale than expert demonstrations of the thing that one wants to achieve. Furthermore, this approach also facilitates the process of data reuse as preference queries can be collected offline and relabeled later; it is harder to correct demonstrations post hoc.

Ultimately, whether we choose to go for IL, preference-based modeling, or direct modeling of objectives and environment comes down to a matter of ease of expressing our intents to a machine. In a sense (which, as far as I know, hasn't been studied in this way very much), this is a human-computer interaction (HCI) problem. Throughout this course, I want to provide you with an array of tools to let you find what's right for your problem by choosing or mixing these techniques together.

# Mathematical Programming Approach

```{epigraph}
The sciences do not try to explain, they hardly even try to interpret, they mainly make models. By a model is meant a mathematical construct which, with the addition of certain verbal interpretations, describes observed phenomena. The justification of such a mathematical construct is solely and precisely that it is expected to work.

-- John von Neumann
```

This course considers two broad categories of environments, each with its own specialized solution methods: deterministic and stochastic environments. Stochastic problems are mathematically more general than their deterministic counterparts. However, despite this generality, it's important to note that algorithms for stochastic problems are not necessarily more powerful than those designed for deterministic ones when used in practice. We should keep in mind that stochasticity and determinism are assumed properties of the world, which we model—perhaps imperfectly—into our algorithms. In this course, we adopt a pragmatic perspective on that matter, and will make assumptions only to the extent that those assumptions help us design algorithms which are ultimately useful in practice: a lesson which we have certainly learned from the success of deep learning methods over the last years. With this pragmatic stance, we start our journey with deterministic discrete-time models. 

## Irrigation Management Example

Resource allocation problems, found across various fields of operations research, are a specific kind of deterministic discrete-time optimal control problems. For example, consider the problem of irrigation management as posed by {cite:t}`Hall1968`, in which a decision maker is tasked with finding the optimal amount of water to allocate throughout the various growth stages of a plant in order to maximize the yield. Clearly, allocating all the water -- thereby flooding the crop -- in the first stage would be a bad idea. Similarly, letting the crop dry out and only watering at the end is also suboptimal. Our solution -- that is a prescription for the amount of water to use at any point in time -- should balance those two extremes. In order to achieve this goal, our system should ideally be informed by the expected growth dynamics of the given crops as a function of the water provided: a process which depends at the very least on physical properties known to influence the soil moisture. The basic model considered in this paper describes the evolution of the moisture content through the equation: 

\begin{align*}
w_{t+1} = w_t+\texttip{\eta}{efficiency} u_t-e_t + \phi_t
\end{align*}

where $\eta$ is a fixed "efficiency" constant determining the soil moisture response to irrigation, and $e_t$ is a known quantity summarizing the effects of water loss due to evaporation and finally $\phi_t$ represents the expected added moisture due to precipitation. 

Furthermore, we should ensure that the total amount of water used throughout the season does not exceed the maximum allowed amount. To avoid situations where our crop receives too little or too much water, we further impose the condition that $w_p \leq w_{t+1} \leq w_f$ for all stages, for some given values of the so-called permanent wilting percentage $w_p$ and field capacity $w_f$. Depending on the moisture content of the soil, the yield will vary up to a maximum value $Y_max$ depending on the water deficiencies incurred throughout the season. The authors make the assumptions that such deficiencies interact multiplicatively across stages such that the total yield is given by
$\left[\prod_{t=1}^N d_t(w_t)\right] Y_{\max}$. Due to the operating cost of watering operations (for example, energy consumption of the pumps, human labor, etc), a more meaningful objective is to maximize $\prod_{t=1}^N d_t\left(w_t\right) Y_{\max } - \sum_{t=1}^N c_t\left(u_t\right)$. 
 The problem specification laid out above can be turned into the following mathematical program:

\begin{alignat*}{2}
\text{minimize} \quad & \sum_{t=1}^N c_t(u_t) - a_N Y_{\max} & \\
\text{such that} \quad 
& w_{t+1} = w_t + \eta u_t - e_t + \phi_t, & \quad & t = 1, \dots, N-1, \\
& q_{t+1} = q_t - u_t, & \quad & t = 1, \dots, N-1, \\
& a_{t+1} = d_t(w_t) a_t, & \quad & t = 1, \dots, N-1, \\
& w_p \leq w_{t} \leq w_f, & \quad & t = 1, \dots, N, \\
& 0 \leq u_t \leq q_t, & \quad & t = 1, \dots, N, \\
& 0 \leq q_t, & \quad & t = 1, \dots, N, \\
& a_0 = 1, & & \\
\text{given} \quad & w_1, q_N. &
\end{alignat*}

The multiplicative form of the objective function coming from the yield term has been eliminated through by adding a new variable, $a_t$, representing the product accumulation of water deficiencies since the beginning of the season. 

Clearly, this model is a simplification of real phenomena at play: that of the physical process of water absorption by a plant through its root system. Many more aspects of the world would have to be included to have a more faithful reproduction of the real process: for example by taking into account the real-time meteorological data, pressure, soil type, solar irradiance, shading and topology of the terrain etc. We could go to the level of even modelling the inner workings of the plant itself to understand exactly how much water will get absorbed. More crucially, our assumption that the water absorption takes place instantaneously at discrete points in time is certainly not true. So should we go back to the drawing board and consider a better model? The answer is that "it depends". It depends on the adequacy of the solution when deployed in the real world, or whether it helped provide insights to the user. Put simply: is the system useful to those who interact with it? Answering this question requires us to think more broadly about our system and how it will interact more broadly with the end users and the society.

